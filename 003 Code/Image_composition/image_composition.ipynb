{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc85786f",
   "metadata": {},
   "source": [
    "# 군용 이미지 합성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc86d1",
   "metadata": {},
   "source": [
    "QWEN으로 드론뷰 여부 판별 후 기록된 내용을 토대로 합성 이미지 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6ed39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------\n",
    "# 패키지 임포트\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    ControlNetModel,\n",
    "    DDPMScheduler,\n",
    "    UNet2DConditionModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "from pipeline_controlnet_inpaint import StableDiffusionControlNetInpaintPipeline\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "from transparent_background import Remover\n",
    "\n",
    "# 재현성을 위한 시드 설정\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 모델 및 파이프라인 로드\n",
    "controlnet = ControlNetModel.from_pretrained('./checkpoints/controlnet/controlnet')\n",
    "\n",
    "def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str, revision: str):\n",
    "    text_encoder_config = PretrainedConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"text_encoder\",\n",
    "        revision=revision,\n",
    "    )\n",
    "    model_class = text_encoder_config.architectures[0]\n",
    "    if model_class == \"CLIPTextModel\":\n",
    "        from transformers import CLIPTextModel\n",
    "        return CLIPTextModel\n",
    "    elif model_class == \"RobertaSeriesModelWithTransformation\":\n",
    "        from diffusers.pipelines.alt_diffusion.modeling_roberta_series import RobertaSeriesModelWithTransformation\n",
    "        return RobertaSeriesModelWithTransformation\n",
    "    else:\n",
    "        raise ValueError(f\"{model_class} is not supported.\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
    "    subfolder=\"tokenizer\",\n",
    "    use_fast=False,\n",
    ")\n",
    "sd_inpainting_model_name = \"stabilityai/stable-diffusion-2-inpainting\"\n",
    "text_encoder_cls = import_model_class_from_model_name_or_path(sd_inpainting_model_name, None)\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(sd_inpainting_model_name, subfolder=\"scheduler\")\n",
    "text_encoder = text_encoder_cls.from_pretrained(sd_inpainting_model_name, subfolder=\"text_encoder\", revision=None)\n",
    "vae = AutoencoderKL.from_pretrained(sd_inpainting_model_name, subfolder=\"vae\", revision=None)\n",
    "unet = UNet2DConditionModel.from_pretrained(sd_inpainting_model_name, subfolder=\"unet\", revision=None)\n",
    "weight_dtype = torch.float32\n",
    "pipeline = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
    "    sd_inpainting_model_name,\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    unet=unet,\n",
    "    controlnet=controlnet,\n",
    "    safety_checker=None,\n",
    "    revision=None,\n",
    "    torch_dtype=weight_dtype,\n",
    ")\n",
    "pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline = pipeline.to('cuda')\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 유틸리티 함수\n",
    "def resize_with_padding(img, expected_size): # 비율을 유지하면서 이미지를 리사이즈하고, 결과가 expected_size와 일치하도록 패딩을 적용\n",
    "    img.thumbnail((expected_size[0], expected_size[1]))\n",
    "    delta_width = expected_size[0] - img.size[0]\n",
    "    delta_height = expected_size[1] - img.size[1]\n",
    "    pad_width = delta_width // 2\n",
    "    pad_height = delta_height // 2\n",
    "    padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
    "    return ImageOps.expand(img, padding)\n",
    "\n",
    "def get_min_area_bbox(mask): # 주어진 PIL 마스크에서 객체의 최소 바운딩 박스를 계산\n",
    "    mask_np = np.array(mask)\n",
    "    if len(mask_np.shape) == 3:\n",
    "        mask_np = cv2.cvtColor(mask_np, cv2.COLOR_RGB2GRAY)\n",
    "    _, binary = cv2.threshold(mask_np, 1, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return (0, 0, mask.width, mask.height)\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    rect = cv2.minAreaRect(cnt)\n",
    "    box = cv2.boxPoints(rect)\n",
    "    box = np.int0(box)\n",
    "    x_min = int(np.min(box[:, 0]))\n",
    "    y_min = int(np.min(box[:, 1]))\n",
    "    x_max = int(np.max(box[:, 0]))\n",
    "    y_max = int(np.max(box[:, 1]))\n",
    "    return (x_min, y_min, x_max, y_max)\n",
    "\n",
    "def classify_image(filename): # 파일 이름에 키워드를 사용하여 이미지를 분류\n",
    "    lower_fname = filename.lower()\n",
    "    if \"bmp3_\" in lower_fname:\n",
    "        return \"bmp3\"\n",
    "    elif \"k200_\" in lower_fname:\n",
    "        return \"k200\"\n",
    "    elif \"k2_\" in lower_fname:\n",
    "        return \"k2\"\n",
    "    elif \"t80_\" in lower_fname:\n",
    "        return \"t80\"\n",
    "    elif \"military truck\" in lower_fname:\n",
    "        return \"military_truck\"\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "# subject를 기반으로 프롬프트 목록을 생성\n",
    "# is_drone_view가 True면 드론 시점에 대한 문장을 추가\n",
    "def get_prompts(subject, is_drone_view=False):\n",
    "    base_prompts = [\n",
    "        f\"A {subject} on the road.\",\n",
    "        f\"A {subject} on the grass.\",\n",
    "        f\"A {subject} on the mountains.\",\n",
    "        f\"A {subject} on a dry dirt field\",\n",
    "        f\"A {subject} on the hill.\",\n",
    "        f\"A {subject} on a snowy road.\",\n",
    "        f\"A {subject} on the dry grassland near a forested hillside.\",\n",
    "        f\"A {subject} floating on a river with trees in the background.\",\n",
    "    ]\n",
    "    if is_drone_view:\n",
    "        base_prompts = [prompt + \" seen from the air.\" for prompt in base_prompts] # 조사한 내용: 드론뷰 이미지인 경우 stable diffusion에 잘 동작하는 프롬프트가 \"see from the air.\"라고 함\n",
    "    return base_prompts\n",
    "\n",
    "# 클래스별 프롬프트 속 객체명(subject) 설정\n",
    "subject_dict = {\n",
    "    \"k2\": \"tank\",\n",
    "    \"t80\": \"tank\",\n",
    "    \"k200\": \"armored car\",\n",
    "    \"bmp3\": \"armored car\",\n",
    "    \"military_truck\": \"military truck\"\n",
    "}\n",
    "\n",
    "# 클래스별 이미지 생성 개수\n",
    "num_seeds_dict = {\n",
    "    \"k2\": 1,\n",
    "    \"k200\": 1,\n",
    "    \"bmp3\": 1,\n",
    "    \"military_truck\": 1,\n",
    "    \"t80\": 1,\n",
    "}\n",
    "\n",
    "# cond_scale 낮을수록 객체 품질 < 배경 품질\n",
    "# cond_scale 높을수록 객체 품질 > 배경 품질\n",
    "cond_scale = 0.5\n",
    "num_inference_steps = 20\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 경로 설정\n",
    "source_folder = r'D:\\py\\AIM\\Projects\\Drone_detection\\OVD\\dataset_datamaker\\train_dataset\\OD_2\\images'\n",
    "base_save_root = r'D:\\py\\AIM\\Projects\\Drone_detection\\OVD\\dataset_datamaker\\train_dataset\\fusion\\train\\images'\n",
    "\n",
    "drone_classification_path = 'qwen으로 분류한 드론뷰 이미지 여부 작성된 .json 경로'\n",
    "if os.path.exists(drone_classification_path):\n",
    "    with open(drone_classification_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        drone_view_results = json.load(f)\n",
    "else:\n",
    "    print(\"Drone view classification JSON file not found. Defaulting to 'Normal View'.\")\n",
    "    drone_view_results = {}\n",
    "\n",
    "# 소스 폴더의 이미지 파일 목록\n",
    "image_files = [f for f in os.listdir(source_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "if not image_files:\n",
    "    print(\"No image files found in the source folder.\")\n",
    "else:\n",
    "    print(f\"Found {len(image_files)} images in the source folder.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 드론 시점 분류에 따라 프롬프트를 조정하여 각 이미지에 대해 합성 실행\n",
    "for img_file in image_files:\n",
    "    cls = classify_image(img_file)\n",
    "    if cls is None:\n",
    "        print(f\"File {img_file} does not meet classification criteria. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # JSON에서 드론 시점 분류 가져오기 ('Drone View' 또는 'Normal View')\n",
    "    drone_result = drone_view_results.get(img_file, \"Normal View\")\n",
    "    is_drone_view = True if drone_result == \"Drone View\" else False\n",
    "\n",
    "    subject = subject_dict.get(cls, \"object\")\n",
    "    prompts = get_prompts(subject, is_drone_view=is_drone_view)\n",
    "    num_seeds = num_seeds_dict.get(cls, 1)\n",
    "\n",
    "    save_folder = os.path.join(base_save_root, cls)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    mask_save_folder = os.path.join(base_save_root, f\"{cls}_mask\")\n",
    "    os.makedirs(mask_save_folder, exist_ok=True)\n",
    "\n",
    "    image_path = os.path.join(source_folder, img_file)\n",
    "    print(f\"\\nProcessing file: {image_path} -> Classified as: {cls}, Drone View: {drone_result}\")\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to open image: {image_path}, error: {e}\")\n",
    "        continue\n",
    "\n",
    "    img = resize_with_padding(img, (640, 640)) # 이미지 리사이즈 -> yolo input size=640*640\n",
    "    base_filename = os.path.splitext(os.path.basename(img_file))[0]\n",
    "\n",
    "    # Remover를 사용하여 객체 마스크 추출\n",
    "    remover = Remover(mode='base')\n",
    "    fg_mask = remover.process(img, type='map')\n",
    "    mask = fg_mask\n",
    "\n",
    "    for _ in range(num_seeds):\n",
    "        selected_prompt = random.choice(prompts)\n",
    "        generator = torch.Generator(device='cuda').manual_seed(42)\n",
    "        margin = 1\n",
    "\n",
    "        # 마스크에서 객체의 최소 바운딩 박스 계산\n",
    "        bbox = get_min_area_bbox(fg_mask)\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "        hw, hh = (x2 - x1) / 2, (y2 - y1) / 2\n",
    "        \n",
    "        # 스케일 변환 시도 최대 횟수\n",
    "        max_attempts = 500\n",
    "        attempt = 0\n",
    "        valid_transform = False\n",
    "\n",
    "        while attempt < max_attempts and not valid_transform:\n",
    "            # 스케일 선택\n",
    "            scale_factor = random.uniform(0.8, 0.9)\n",
    "            new_hw, new_hh = scale_factor * hw, scale_factor * hh\n",
    "            dx_min = margin - (cx - new_hw)\n",
    "            dx_max = mask.width - margin - (cx + new_hw)\n",
    "            dy_min = margin - (cy - new_hh)\n",
    "            dy_max = mask.height - margin - (cy + new_hh)\n",
    "            if dx_max < dx_min or dy_max < dy_min:\n",
    "                attempt += 1\n",
    "                continue\n",
    "            dx = random.uniform(dx_min, dx_max)\n",
    "            dy = random.uniform(dy_min, dy_max)\n",
    "            t_x = (1 - scale_factor) * cx + dx\n",
    "            t_y = (1 - scale_factor) * cy + dy\n",
    "            forward_matrix = (scale_factor, 0, t_x, 0, scale_factor, t_y)\n",
    "            inv_matrix = (1/scale_factor, 0, -t_x/scale_factor, 0, 1/scale_factor, -t_y/scale_factor)\n",
    "            shifted_mask = mask.transform(mask.size, Image.AFFINE, inv_matrix, fillcolor=0)\n",
    "            transformed_bbox = shifted_mask.getbbox()\n",
    "            if transformed_bbox is None:\n",
    "                attempt += 1\n",
    "                continue\n",
    "            tx1, ty1, tx2, ty2 = transformed_bbox\n",
    "            if tx1 >= margin and ty1 >= margin and tx2 <= mask.width - margin and ty2 <= mask.height - margin:\n",
    "                valid_transform = True\n",
    "            else:\n",
    "                attempt += 1\n",
    "\n",
    "        if not valid_transform:\n",
    "            # 유효한 변환을 찾지 못한 경우 순수 이동 변환 사용\n",
    "            print(\"Could not find a valid affine transform. Using pure translation instead.\")\n",
    "            allowed_dx_min = margin - x1\n",
    "            allowed_dx_max = mask.width - margin - x2\n",
    "            allowed_dy_min = margin - y1\n",
    "            allowed_dy_max = mask.height - margin - y2\n",
    "            dx = random.uniform(allowed_dx_min, allowed_dx_max)\n",
    "            dy = random.uniform(allowed_dy_min, allowed_dy_max)\n",
    "            forward_matrix = (1.0, 0, dx, 0, 1.0, dy)\n",
    "            inv_matrix = (1.0, 0, -dx, 0, 1.0, -dy)\n",
    "            shifted_mask = mask.transform(mask.size, Image.AFFINE, inv_matrix, fillcolor=0)\n",
    "\n",
    "        shifted_img = img.transform(img.size, Image.AFFINE, inv_matrix, fillcolor=0)\n",
    "        invert_mask = ImageOps.invert(shifted_mask)\n",
    "\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            generated_image = pipeline(\n",
    "                prompt=selected_prompt,\n",
    "                image=shifted_img,\n",
    "                mask_image=invert_mask,\n",
    "                control_image=invert_mask,\n",
    "                num_images_per_prompt=1,\n",
    "                generator=generator,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guess_mode=False,\n",
    "                controlnet_conditioning_scale=cond_scale\n",
    "            ).images[0]\n",
    "\n",
    "        # 합성 이미지, 마스크 저장\n",
    "        ext = os.path.splitext(img_file)[1] or '.jpg'\n",
    "        save_path = os.path.join(save_folder, f\"{base_filename}_42{ext}\")\n",
    "        generated_image.save(save_path)\n",
    "        print(f\"Seed 42 | Prompt: {selected_prompt} -> Saved image to {save_path}\")\n",
    "\n",
    "        mask_save_path = os.path.join(mask_save_folder, f\"{base_filename}_42{ext}\")\n",
    "        shifted_mask.save(mask_save_path)\n",
    "        print(f\"Saved shifted mask to {mask_save_path}\")\n",
    "\n",
    "print(\"Image synthesis completed for all images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad312cb",
   "metadata": {},
   "source": [
    "소스 이미지 일부 샘플링하여 합성하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97e63f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pipeline_controlnet_inpaint'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     AutoencoderKL,\n\u001b[0;32m     11\u001b[0m     ControlNetModel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     UniPCMultistepScheduler,\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpipeline_controlnet_inpaint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableDiffusionControlNetInpaintPipeline\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, PretrainedConfig\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pipeline_controlnet_inpaint'"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------------\n",
    "# 패키지 임포트\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    ControlNetModel,\n",
    "    DDPMScheduler,\n",
    "    UNet2DConditionModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "from pipeline_controlnet_inpaint import StableDiffusionControlNetInpaintPipeline\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "from transparent_background import Remover\n",
    "\n",
    "# ---------------------- 재현성을 위한 시드 설정 ----------------------\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 모델 및 파이프라인 로드\n",
    "controlnet = ControlNetModel.from_pretrained('./checkpoints/controlnet/controlnet')\n",
    "\n",
    "def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str, revision: str):\n",
    "    text_encoder_config = PretrainedConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"text_encoder\",\n",
    "        revision=revision,\n",
    "    )\n",
    "    model_class = text_encoder_config.architectures[0]\n",
    "    if model_class == \"CLIPTextModel\":\n",
    "        from transformers import CLIPTextModel\n",
    "        return CLIPTextModel\n",
    "    elif model_class == \"RobertaSeriesModelWithTransformation\":\n",
    "        from diffusers.pipelines.alt_diffusion.modeling_roberta_series import RobertaSeriesModelWithTransformation\n",
    "        return RobertaSeriesModelWithTransformation\n",
    "    else:\n",
    "        raise ValueError(f\"{model_class} is not supported.\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-inpainting\",\n",
    "    subfolder=\"tokenizer\",\n",
    "    use_fast=False,\n",
    ")\n",
    "sd_inpainting_model_name = \"stabilityai/stable-diffusion-2-inpainting\"\n",
    "text_encoder_cls = import_model_class_from_model_name_or_path(sd_inpainting_model_name, None)\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(sd_inpainting_model_name, subfolder=\"scheduler\")\n",
    "text_encoder = text_encoder_cls.from_pretrained(sd_inpainting_model_name, subfolder=\"text_encoder\", revision=None)\n",
    "vae = AutoencoderKL.from_pretrained(sd_inpainting_model_name, subfolder=\"vae\", revision=None)\n",
    "unet = UNet2DConditionModel.from_pretrained(sd_inpainting_model_name, subfolder=\"unet\", revision=None)\n",
    "weight_dtype = torch.float32\n",
    "pipeline = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
    "    sd_inpainting_model_name,\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    unet=unet,\n",
    "    controlnet=controlnet,\n",
    "    safety_checker=None,\n",
    "    revision=None,\n",
    "    torch_dtype=weight_dtype,\n",
    ")\n",
    "pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "pipeline = pipeline.to('cuda')\n",
    "pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 유틸리티 함수\n",
    "def resize_with_padding(img, expected_size):\n",
    "    \"\"\"\n",
    "    비율을 유지하면서 이미지를 리사이즈 후, 중심에 회색패딩을 넣어 expected_size에 맞춤\n",
    "    \"\"\"\n",
    "    img.thumbnail((expected_size[0], expected_size[1]))\n",
    "    delta_width = expected_size[0] - img.size[0]\n",
    "    delta_height = expected_size[1] - img.size[1]\n",
    "    pad_width = delta_width // 2\n",
    "    pad_height = delta_height // 2\n",
    "    padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
    "    return ImageOps.expand(img, padding)\n",
    "\n",
    "def get_min_area_bbox(mask):\n",
    "    \"\"\"\n",
    "    주어진 PIL 마스크에서 컨투어를 찾아 최소 바운딩 박스를 반환\n",
    "    \"\"\"\n",
    "    mask_np = np.array(mask)\n",
    "    if len(mask_np.shape) == 3:\n",
    "        mask_np = cv2.cvtColor(mask_np, cv2.COLOR_RGB2GRAY)\n",
    "    _, binary = cv2.threshold(mask_np, 1, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return (0, 0, mask.width, mask.height)\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    rect = cv2.minAreaRect(cnt)\n",
    "    box = cv2.boxPoints(rect)\n",
    "    box = np.int0(box)\n",
    "    x_min = int(np.min(box[:, 0]))\n",
    "    y_min = int(np.min(box[:, 1]))\n",
    "    x_max = int(np.max(box[:, 0]))\n",
    "    y_max = int(np.max(box[:, 1]))\n",
    "    return (x_min, y_min, x_max, y_max)\n",
    "\n",
    "def classify_image(filename):\n",
    "    \"\"\"\n",
    "    파일명에 포함된 키워드를 기준으로 클래스 반환\n",
    "    - bmp3_ -> \"bmp3\"\n",
    "    - t80_  -> \"t80\"\n",
    "    - truck -> \"military_truck\"\n",
    "    \"\"\"\n",
    "    lower_fname = filename.lower()\n",
    "    if \"bmp3_\" in lower_fname:\n",
    "        return \"bmp3\"\n",
    "    elif \"t80_\" in lower_fname:\n",
    "        return \"t80\"\n",
    "    elif \"truck\" in lower_fname:\n",
    "        return \"military_truck\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_prompts(subject, is_drone_view=False):\n",
    "    \"\"\"\n",
    "    subject (tank, armored car, military truck 등)를 사용하여 기본 프롬프트 목록 생성.\n",
    "    is_drone_view=True이면 끝에 'seen from the air.' 추가\n",
    "    \"\"\"\n",
    "    base_prompts = [\n",
    "        f\"A {subject} on the road.\",\n",
    "        f\"A {subject} on the grass.\",\n",
    "        f\"A {subject} on the mountains.\",\n",
    "        f\"A {subject} on a dry dirt field\",\n",
    "        f\"A {subject} on the hill.\",\n",
    "        f\"A {subject} on a snowy road.\",\n",
    "        f\"A {subject} on the dry grassland near a forested hillside.\",\n",
    "        f\"A {subject} floating on a river with trees in the background.\",\n",
    "    ]\n",
    "    if is_drone_view:\n",
    "        base_prompts = [prompt + \" seen from the air.\" for prompt in base_prompts]\n",
    "    return base_prompts\n",
    "\n",
    "# 클래스별 subject 매핑\n",
    "subject_dict = {\n",
    "    \"t80\": \"tank\",\n",
    "    \"bmp3\": \"armored car\",\n",
    "    \"military_truck\": \"military truck\"\n",
    "}\n",
    "\n",
    "# 클래스별 인퍼런스 파라미터 (여기서는 1)\n",
    "num_seeds_dict = {\n",
    "    \"bmp3\": 1,\n",
    "    \"military_truck\": 1,\n",
    "    \"t80\": 1,\n",
    "}\n",
    "\n",
    "# ControlNet 인퍼런스 파라미터\n",
    "cond_scale = 0.5\n",
    "num_inference_steps = 20\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 경로 설정 (실제 환경에 맞춰 수정)\n",
    "source_folder = r'D:\\py\\AIM\\Projects\\Drone_detection\\OVD\\dataset_datamaker\\train_dataset\\OD_2\\images'\n",
    "base_save_root = r'D:\\py\\AIM\\Projects\\Drone_detection\\OVD\\dataset_datamaker\\train_dataset\\fusion\\train\\images'\n",
    "\n",
    "# Qwen 분류 스크립트에서 생성한 JSON 파일 경로 (반드시 실제 JSON 경로로 바꿔주세요)\n",
    "drone_classification_path = r'D:\\py\\AIM\\Projects\\Drone_detection\\OVD\\dataset_datamaker\\train_dataset\\fusion\\train\\drone_view_classification.json'\n",
    "\n",
    "if os.path.exists(drone_classification_path):\n",
    "    with open(drone_classification_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        drone_view_results = json.load(f)\n",
    "else:\n",
    "    print(f\"JSON 파일을 찾을 수 없습니다: {drone_classification_path}\")\n",
    "    drone_view_results = {}\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 소스 폴더의 이미지 파일 목록(.jpg, .jpeg, .png, .bmp)\n",
    "image_files = [f for f in os.listdir(source_folder) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\"))]\n",
    "if not image_files:\n",
    "    print(\"소스 폴더에 이미지 파일이 없습니다.\")\n",
    "else:\n",
    "    print(f\"소스 폴더에서 {len(image_files)}개의 이미지를 찾았습니다.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1. 클래스별 파일 리스트 생성\n",
    "files_by_class = {\n",
    "    \"bmp3\": [],\n",
    "    \"t80\": [],\n",
    "    \"military_truck\": []\n",
    "}\n",
    "\n",
    "for fname in image_files:\n",
    "    cls = classify_image(fname)\n",
    "    if cls in files_by_class:\n",
    "        files_by_class[cls].append(fname)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 2. 픽셀 면적 기준(전체 640*640 = 409600픽셀, 20% = 81920픽셀)으로 유효한 샘플 100장씩 수집\n",
    "threshold_ratio = 0.20\n",
    "total_pixels = 640 * 640\n",
    "threshold_pixels = int(total_pixels * threshold_ratio)  # 81920\n",
    "\n",
    "sampled_files = []\n",
    "\n",
    "for cls_name, file_list in files_by_class.items():\n",
    "    if len(file_list) < 100:\n",
    "        raise ValueError(f\"클래스 '{cls_name}'의 이미지가 100장 미만입니다. 현재 개수: {len(file_list)}\")\n",
    "    \n",
    "    # 시드 고정 후 셔플\n",
    "    random.seed(42)\n",
    "    random.shuffle(file_list)\n",
    "\n",
    "    valid_samples = []\n",
    "    for fname in file_list:\n",
    "        image_path = os.path.join(source_folder, fname)\n",
    "        try:\n",
    "            img = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: 이미지 열기 실패({fname}): {e}\")\n",
    "            continue\n",
    "\n",
    "        # 640x640으로 패딩 리사이즈\n",
    "        img_resized = resize_with_padding(img, (640, 640))\n",
    "        \n",
    "        # 마스크 추출\n",
    "        remover = Remover(mode='base')\n",
    "        fg_mask = remover.process(img_resized, type='map')\n",
    "        mask_np = np.array(fg_mask)\n",
    "        if len(mask_np.shape) == 3:\n",
    "            mask_np = cv2.cvtColor(mask_np, cv2.COLOR_RGB2GRAY)\n",
    "        _, binary_mask = cv2.threshold(mask_np, 1, 255, cv2.THRESH_BINARY)\n",
    "        fg_count = np.count_nonzero(binary_mask)\n",
    "\n",
    "        # 유효 여부 판단: foreground 픽셀 > threshold\n",
    "        if fg_count > threshold_pixels:\n",
    "            valid_samples.append(fname)\n",
    "\n",
    "        if len(valid_samples) == 100:\n",
    "            break\n",
    "\n",
    "    if len(valid_samples) < 100:\n",
    "        raise ValueError(f\"클래스 '{cls_name}'에서 유효한 샘플 100장을 찾지 못했습니다. (찾은 개수: {len(valid_samples)})\")\n",
    "    \n",
    "    sampled_files.extend(valid_samples)\n",
    "    print(f\"클래스 '{cls_name}' - 유효 샘플 100장 수집 완료.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 3. 샘플링된 파일들에 대해 합성 수행 및 메타데이터 기록\n",
    "generated_metadata = {}  # key: 상대 경로(클래스/파일명), value: 사용된 프롬프트\n",
    "\n",
    "for img_file in sampled_files:\n",
    "    cls = classify_image(img_file)  # 반드시 bmp3, t80, 또는 military_truck 중 하나여야 함\n",
    "\n",
    "    # JSON에서 드론뷰 분류 결과 가져오기 (키: 파일명, 값: \"Drone View\" 또는 \"Normal View\")\n",
    "    drone_result = drone_view_results.get(img_file, \"Normal View\")\n",
    "    is_drone_view = True if drone_result == \"Drone View\" else False\n",
    "\n",
    "    subject = subject_dict.get(cls, \"object\")\n",
    "    prompts = get_prompts(subject, is_drone_view=is_drone_view)\n",
    "    num_seeds = num_seeds_dict.get(cls, 1)\n",
    "\n",
    "    # 합성된 이미지 및 마스크를 저장할 폴더 생성\n",
    "    save_folder = os.path.join(base_save_root, cls)\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    mask_save_folder = os.path.join(base_save_root, f\"{cls}_mask\")\n",
    "    os.makedirs(mask_save_folder, exist_ok=True)\n",
    "\n",
    "    image_path = os.path.join(source_folder, img_file)\n",
    "    print(f\"\\nProcessing file: {image_path} -> Classified as: {cls}, Drone View: {drone_result}\")\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"이미지 열기 실패: {image_path}, 오류: {e}\")\n",
    "        continue\n",
    "\n",
    "    # YOLO 입력 크기에 맞춰 패딩 리사이즈 (640x640)\n",
    "    img = resize_with_padding(img, (640, 640))\n",
    "    base_filename = os.path.splitext(os.path.basename(img_file))[0]\n",
    "\n",
    "    # Remover를 사용해 전경 마스크 추출\n",
    "    remover = Remover(mode='base')\n",
    "    fg_mask = remover.process(img, type='map')\n",
    "    mask = fg_mask\n",
    "\n",
    "    # num_seeds 만큼 합성 수행 (여기서는 1회)\n",
    "    for _ in range(num_seeds):\n",
    "        selected_prompt = random.choice(prompts)\n",
    "        generator = torch.Generator(device='cuda').manual_seed(42)\n",
    "        margin = 1\n",
    "\n",
    "        # 객체의 최소 바운딩 박스 계산\n",
    "        bbox = get_min_area_bbox(fg_mask)\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "        hw, hh = (x2 - x1) / 2, (y2 - y1) / 2\n",
    "        \n",
    "        # 스케일 변환 시도\n",
    "        max_attempts = 500\n",
    "        attempt = 0\n",
    "        valid_transform = False\n",
    "\n",
    "        while attempt < max_attempts and not valid_transform:\n",
    "            scale_factor = random.uniform(0.8, 0.9)\n",
    "            new_hw, new_hh = scale_factor * hw, scale_factor * hh\n",
    "            dx_min = margin - (cx - new_hw)\n",
    "            dx_max = mask.width - margin - (cx + new_hw)\n",
    "            dy_min = margin - (cy - new_hh)\n",
    "            dy_max = mask.height - margin - (cy + new_hh)\n",
    "            if dx_max < dx_min or dy_max < dy_min:\n",
    "                attempt += 1\n",
    "                continue\n",
    "            dx = random.uniform(dx_min, dx_max)\n",
    "            dy = random.uniform(dy_min, dy_max)\n",
    "            t_x = (1 - scale_factor) * cx + dx\n",
    "            t_y = (1 - scale_factor) * cy + dy\n",
    "            forward_matrix = (scale_factor, 0, t_x, 0, scale_factor, t_y)\n",
    "            inv_matrix = (1/scale_factor, 0, -t_x/scale_factor, 0, 1/scale_factor, -t_y/scale_factor)\n",
    "            shifted_mask = mask.transform(mask.size, Image.AFFINE, inv_matrix, fillcolor=0)\n",
    "            transformed_bbox = shifted_mask.getbbox()\n",
    "            if transformed_bbox is None:\n",
    "                attempt += 1\n",
    "                continue\n",
    "            tx1, ty1, tx2, ty2 = transformed_bbox\n",
    "            if tx1 >= margin and ty1 >= margin and tx2 <= mask.width - margin and ty2 <= mask.height - margin:\n",
    "                valid_transform = True\n",
    "            else:\n",
    "                attempt += 1\n",
    "\n",
    "        if not valid_transform:\n",
    "            # 유효한 변환을 찾지 못했을 때 순수 이동 변환 적용\n",
    "            print(\"유효한 어파인 변환을 찾지 못했습니다. 순수 이동 변환으로 대체합니다.\")\n",
    "            allowed_dx_min = margin - x1\n",
    "            allowed_dx_max = mask.width - margin - x2\n",
    "            allowed_dy_min = margin - y1\n",
    "            allowed_dy_max = mask.height - margin - y2\n",
    "            dx = random.uniform(allowed_dx_min, allowed_dx_max)\n",
    "            dy = random.uniform(allowed_dy_min, allowed_dy_max)\n",
    "            forward_matrix = (1.0, 0, dx, 0, 1.0, dy)\n",
    "            inv_matrix = (1.0, 0, -dx, 0, 1.0, -dy)\n",
    "            shifted_mask = mask.transform(mask.size, Image.AFFINE, inv_matrix, fillcolor=0)\n",
    "\n",
    "        # 이미지를 이동/스케일 변환\n",
    "        shifted_img = img.transform(img.size, Image.AFFINE, inv_matrix, fillcolor=0)\n",
    "        invert_mask = ImageOps.invert(shifted_mask)\n",
    "\n",
    "        # Stable Diffusion + ControlNet 합성\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            generated_image = pipeline(\n",
    "                prompt=selected_prompt,\n",
    "                image=shifted_img,\n",
    "                mask_image=invert_mask,\n",
    "                control_image=invert_mask,\n",
    "                num_images_per_prompt=1,\n",
    "                generator=generator,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guess_mode=False,\n",
    "                controlnet_conditioning_scale=cond_scale\n",
    "            ).images[0]\n",
    "\n",
    "        # 합성 이미지 및 마스크 저장\n",
    "        ext = os.path.splitext(img_file)[1] or '.jpg'\n",
    "        save_path = os.path.join(save_folder, f\"{base_filename}_42{ext}\")\n",
    "        generated_image.save(save_path)\n",
    "        print(f\"Seed 42 | Prompt: {selected_prompt} -> Saved image to {save_path}\")\n",
    "\n",
    "        mask_save_path = os.path.join(mask_save_folder, f\"{base_filename}_42{ext}\")\n",
    "        shifted_mask.save(mask_save_path)\n",
    "        print(f\"Saved shifted mask to {mask_save_path}\")\n",
    "\n",
    "        # 메타데이터에 기록 (상대 경로로 저장)\n",
    "        rel_path = os.path.relpath(save_path, base_save_root).replace(\"\\\\\", \"/\")\n",
    "        generated_metadata[rel_path] = selected_prompt\n",
    "\n",
    "print(\"샘플링된 모든 이미지에 대한 합성이 완료되었습니다.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 4. 메타데이터를 JSON 파일로 저장\n",
    "metadata_save_path = os.path.join(base_save_root, \"generated_images_prompts.json\")\n",
    "with open(metadata_save_path, \"w\", encoding=\"utf-8\") as meta_f:\n",
    "    json.dump(generated_metadata, meta_f, ensure_ascii=False, indent=4)\n",
    "print(f\"생성된 이미지 메타데이터가 저장되었습니다: {metadata_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv386",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
